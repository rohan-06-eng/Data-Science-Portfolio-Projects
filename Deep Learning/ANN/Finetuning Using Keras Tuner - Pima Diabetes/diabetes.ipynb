{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the ANN Architecture with Keras Tuner\n",
    "#### The function build_model defines a dynamic Artificial Neural Network (ANN) architecture designed for binary classification tasks. It leverages Keras Tuner to fine-tune various hyperparameters across multiple layers to optimize the model's performance. Below is a detailed description of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\rajro\\anaconda3\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras-tuner) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from optree->keras->keras-tuner) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rajro\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "!pip install keras-tuner\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = [\n",
    "    \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \n",
    "    \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"\n",
    "]\n",
    "data = pd.read_csv(url, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input Layer\n",
    "1. The input layer acts as the entry point of the model, where features from the dataset are fed into the network.\n",
    "2. The number of neurons (units) in this layer is a tunable parameter, ranging from 8 to 128 in steps of 8. This flexibility allows the tuner to explore configurations that balance complexity and performance.\n",
    "3. The activation function is another tunable parameter, which can be relu, tanh, or sigmoid. These functions introduce non-linearity and help the model learn complex patterns.\n",
    "4. The input shape is fixed and depends on the number of features in the training dataset (X_train.shape[1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hidden Layers\n",
    "1. Hidden layers are responsible for feature extraction and pattern recognition.\n",
    "2. The number of hidden layers is tunable between 1 and 3, allowing for customization based on the complexity of the data.\n",
    "3. Each hidden layer has:\n",
    "    - Number of Neurons: Tunable between 8 to 128 in steps of 8, enabling exploration of model depth.\n",
    "    - Activation Function: Tunable between relu, tanh, or sigmoid to adjust how the neurons process the data.\n",
    "    - Dropout: A regularization technique to prevent overfitting. The dropout rate is tunable between 0.0 and 0.5, where a higher dropout rate  means more neurons are randomly disabled during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output Layer\n",
    "1. The output layer has a single neuron since the task is binary classification.\n",
    "2. A sigmoid activation function is used to convert the output into a probability value between 0 and 1, representing the likelihood of the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Compilation\n",
    "1. Optimizer: The model uses the Adam optimizer, a popular choice for its ability to handle sparse gradients and adapt learning rates during training. The learning rate is tunable, allowing values from 0.001 to 0.5.\n",
    "2. Loss Function: Binary crossentropy is used as it is the standard for binary classification tasks. It measures the difference between predicted probabilities and actual labels.\n",
    "3. Metrics: The model tracks accuracy, providing a straightforward measure of performance during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for building the model (used by Keras Tuner)\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units_input', min_value=8, max_value=128, step=8),\n",
    "        activation=hp.Choice('activation_input', ['relu', 'tanh', 'sigmoid']),\n",
    "        input_shape=(X_train.shape[1],)\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(hp.Int('num_hidden_layers', 1, 3)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=8, max_value=128, step=8),\n",
    "            activation=hp.Choice(f'activation_{i}', ['relu', 'tanh', 'sigmoid'])\n",
    "        ))\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice('learning_rate', [0.001,0.005,0.05,0.1,0.2,0.5])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_results\\diabetes_ann_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to train per combination\n",
    "    directory='tuner_results',\n",
    "    project_name='diabetes_ann_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of units in the first layer is 32.\n",
      "The optimal activation function for the input layer is sigmoid.\n",
      "The optimal learning rate is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters and model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first layer is {best_hps.get('units_input')}.\n",
    "The optimal activation function for the input layer is {best_hps.get('activation_input')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajro\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 - 5s - 321ms/step - accuracy: 0.6110 - loss: 0.7217 - val_accuracy: 0.7398 - val_loss: 0.5830\n",
      "Epoch 2/50\n",
      "16/16 - 0s - 13ms/step - accuracy: 0.7088 - loss: 0.5798 - val_accuracy: 0.7398 - val_loss: 0.5215\n",
      "Epoch 3/50\n",
      "16/16 - 0s - 11ms/step - accuracy: 0.7556 - loss: 0.5152 - val_accuracy: 0.7886 - val_loss: 0.4827\n",
      "Epoch 4/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7658 - loss: 0.4857 - val_accuracy: 0.7805 - val_loss: 0.4755\n",
      "Epoch 5/50\n",
      "16/16 - 0s - 13ms/step - accuracy: 0.7699 - loss: 0.4862 - val_accuracy: 0.7724 - val_loss: 0.4927\n",
      "Epoch 6/50\n",
      "16/16 - 0s - 14ms/step - accuracy: 0.7637 - loss: 0.4736 - val_accuracy: 0.7724 - val_loss: 0.4792\n",
      "Epoch 7/50\n",
      "16/16 - 0s - 12ms/step - accuracy: 0.7576 - loss: 0.4797 - val_accuracy: 0.7480 - val_loss: 0.5176\n",
      "Epoch 8/50\n",
      "16/16 - 0s - 18ms/step - accuracy: 0.7780 - loss: 0.4894 - val_accuracy: 0.7642 - val_loss: 0.4824\n",
      "Epoch 9/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7597 - loss: 0.4748 - val_accuracy: 0.7398 - val_loss: 0.5747\n",
      "Epoch 10/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7699 - loss: 0.4878 - val_accuracy: 0.7561 - val_loss: 0.4809\n",
      "Epoch 11/50\n",
      "16/16 - 0s - 14ms/step - accuracy: 0.7678 - loss: 0.4753 - val_accuracy: 0.7724 - val_loss: 0.4812\n",
      "Epoch 12/50\n",
      "16/16 - 0s - 15ms/step - accuracy: 0.7760 - loss: 0.4538 - val_accuracy: 0.7642 - val_loss: 0.4796\n",
      "Epoch 13/50\n",
      "16/16 - 0s - 12ms/step - accuracy: 0.7841 - loss: 0.4498 - val_accuracy: 0.7642 - val_loss: 0.5211\n",
      "Epoch 14/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7678 - loss: 0.4698 - val_accuracy: 0.7480 - val_loss: 0.4810\n",
      "Epoch 15/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7658 - loss: 0.4630 - val_accuracy: 0.7642 - val_loss: 0.4772\n",
      "Epoch 16/50\n",
      "16/16 - 0s - 13ms/step - accuracy: 0.7923 - loss: 0.4495 - val_accuracy: 0.7480 - val_loss: 0.4917\n",
      "Epoch 17/50\n",
      "16/16 - 0s - 11ms/step - accuracy: 0.7963 - loss: 0.4713 - val_accuracy: 0.7724 - val_loss: 0.4763\n",
      "Epoch 18/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7841 - loss: 0.4515 - val_accuracy: 0.7480 - val_loss: 0.4866\n",
      "Epoch 19/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7760 - loss: 0.4479 - val_accuracy: 0.7561 - val_loss: 0.4805\n",
      "Epoch 20/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7984 - loss: 0.4408 - val_accuracy: 0.7724 - val_loss: 0.4745\n",
      "Epoch 21/50\n",
      "16/16 - 0s - 11ms/step - accuracy: 0.7780 - loss: 0.4667 - val_accuracy: 0.7642 - val_loss: 0.4783\n",
      "Epoch 22/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7800 - loss: 0.4384 - val_accuracy: 0.7561 - val_loss: 0.4894\n",
      "Epoch 23/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7678 - loss: 0.4543 - val_accuracy: 0.7561 - val_loss: 0.4856\n",
      "Epoch 24/50\n",
      "16/16 - 0s - 12ms/step - accuracy: 0.7821 - loss: 0.4585 - val_accuracy: 0.7724 - val_loss: 0.4784\n",
      "Epoch 25/50\n",
      "16/16 - 0s - 13ms/step - accuracy: 0.7923 - loss: 0.4454 - val_accuracy: 0.7642 - val_loss: 0.4766\n",
      "Epoch 26/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.8004 - loss: 0.4340 - val_accuracy: 0.7642 - val_loss: 0.4875\n",
      "Epoch 27/50\n",
      "16/16 - 0s - 13ms/step - accuracy: 0.7760 - loss: 0.4563 - val_accuracy: 0.7642 - val_loss: 0.4738\n",
      "Epoch 28/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.8065 - loss: 0.4418 - val_accuracy: 0.7480 - val_loss: 0.4784\n",
      "Epoch 29/50\n",
      "16/16 - 0s - 11ms/step - accuracy: 0.7923 - loss: 0.4431 - val_accuracy: 0.7480 - val_loss: 0.4938\n",
      "Epoch 30/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7821 - loss: 0.4449 - val_accuracy: 0.7480 - val_loss: 0.4791\n",
      "Epoch 31/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7800 - loss: 0.4419 - val_accuracy: 0.7642 - val_loss: 0.5059\n",
      "Epoch 32/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.8004 - loss: 0.4371 - val_accuracy: 0.7480 - val_loss: 0.4831\n",
      "Epoch 33/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7902 - loss: 0.4361 - val_accuracy: 0.7480 - val_loss: 0.5100\n",
      "Epoch 34/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7943 - loss: 0.4419 - val_accuracy: 0.7561 - val_loss: 0.4890\n",
      "Epoch 35/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7902 - loss: 0.4551 - val_accuracy: 0.7561 - val_loss: 0.4869\n",
      "Epoch 36/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7984 - loss: 0.4459 - val_accuracy: 0.7398 - val_loss: 0.4895\n",
      "Epoch 37/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7943 - loss: 0.4478 - val_accuracy: 0.7642 - val_loss: 0.4862\n",
      "Epoch 38/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7923 - loss: 0.4423 - val_accuracy: 0.7480 - val_loss: 0.4810\n",
      "Epoch 39/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7862 - loss: 0.4463 - val_accuracy: 0.7561 - val_loss: 0.4831\n",
      "Epoch 40/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7963 - loss: 0.4425 - val_accuracy: 0.7398 - val_loss: 0.4858\n",
      "Epoch 41/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7862 - loss: 0.4345 - val_accuracy: 0.7642 - val_loss: 0.4988\n",
      "Epoch 42/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7923 - loss: 0.4308 - val_accuracy: 0.7317 - val_loss: 0.4871\n",
      "Epoch 43/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7862 - loss: 0.4401 - val_accuracy: 0.7398 - val_loss: 0.4927\n",
      "Epoch 44/50\n",
      "16/16 - 0s - 9ms/step - accuracy: 0.7984 - loss: 0.4206 - val_accuracy: 0.7398 - val_loss: 0.5037\n",
      "Epoch 45/50\n",
      "16/16 - 0s - 10ms/step - accuracy: 0.7963 - loss: 0.4368 - val_accuracy: 0.7398 - val_loss: 0.4835\n",
      "Epoch 46/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.8045 - loss: 0.4309 - val_accuracy: 0.7642 - val_loss: 0.4964\n",
      "Epoch 47/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.8024 - loss: 0.4303 - val_accuracy: 0.7317 - val_loss: 0.4876\n",
      "Epoch 48/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7902 - loss: 0.4295 - val_accuracy: 0.7561 - val_loss: 0.4930\n",
      "Epoch 49/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7963 - loss: 0.4332 - val_accuracy: 0.7480 - val_loss: 0.4997\n",
      "Epoch 50/50\n",
      "16/16 - 0s - 8ms/step - accuracy: 0.7882 - loss: 0.4427 - val_accuracy: 0.7480 - val_loss: 0.4858\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
